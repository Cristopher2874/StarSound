# StarSound Project Details: almost complete path of project

*Section with description in detail of the creation of the project*

*Challenge: [Symphony of the stars...](https://www.spaceappschallenge.org/nasa-space-apps-2024/challenges/symphony-of-the-stars-harmonizing-the-james-webb-space-telescope-in-music-and-images/?tab=resources)*

*Under Nasa SpaceApps 2024 context*

--

## Table of Contents
1. [Contributors](#contributors)
2. [Objective](#objective)
3. [Challenge Selection](#challenge-selection)
4. [Concept of the Project](#concept-of-the-project)
5. [Planning steps](#planning-steps)
6. [General App Workflow](#workflow)
7. [Value of the Project](#value-of-the-project)
8. [Technical Sheet](#technical-steps)

---

## Contributors
- [**Cristopher Hernandez**](https://github.com/Cristopher2874/) - Project lead and developer
- [**Jesús Eduardo Ramírez**](https://github.com/Jesus-e) - Project developer
- [**Javier Alejandro Patiño**](https://github.com/Alexpm3435) - Project developer

---

## Objective

**The objective of the app is to promote sharing knowledge with community by music and image content based on the James Webb Telescope mission**

## Challenge selection

We as team InCtrl decided to take a leap on the spacial science and focus on a recurrent problem: how to share the amaizing science to people that is not familiarized with?

We decided to take the challenge of the "Symphony of the stars" because we think that music and images are tha main stimulators of today's society, so we decided to use a presentation 
that is familiar with the user.

The idea started by giving the user complete control of the content generation, so the children could play with sounds by hours and the creativity and advanced logic from the
grown-up public could be also stimulated.

So here the idea born. StarSound, an app similar to the infinite scrolling apps that are present in social media, but now, with images and sounds generated using James Webb Telescope
data.

## Concept of the project

The main idea of the StarSound App is to have a friendly interface that allows the user to select a JWT image, then, the App will place buttons on the zones where the image
contains more light, making reference to galaxies, bodies and planets presents in the image. Once the buttons are set, the user can touch the screen and the buttons pressed will
reproduce a sound, this sound is a note or a chord in an instrument that fits the category of relaxing and atmosferic. This sounds where proposed because is the kind of music
that is generally associated with the space and astronomic bodies. When the user is interacting with the image, ¡the image iself has become a musical instrument! Each button
placed in the stars of gallaxies inside the image is like a key in a keyboard or a string in a guitar. As the button is randomly assigned, the sound could change for one time and the 
next one.

when the user is happy with the sounds that the image produces, it is possible to record a short video showing the pattern and music generated by the user. Then, this video could be
uploaded to the app in order to share the creation with other people in community. As the users add their own work, the possibilities are endless, each image has random sounds and the
user can create a new pattern and music each time the user interacts with the image. 

The main aim of the project is to encourage people to use this technology and by looking at the musical creations of other users around the world, be inspired to create their
own music and share it with the community, and in the process, learn about the JWT mission and main facts.

## Planning steps

- **Data collection**: We will collect data from the James Webb Telescope, mainly, learn about the mission and key details in the official website
- **Image Access**: Download and revise the available resources from the JWT official site to implement the images in the project
- **Project Core**: Use Flutter to create the code of the empty app
- **Image Processing**: Develop Python script to recognize the main highlighted zones of the image
- **JSON mapping**: create a json file to store the coordinates of the places where the image has bright bodies
- **Firebase Implementation**: create a database to storage the images and the json files and retrieve them from the app.
- **Music Stock**: Download and implement the sounds
- **Main Home Screen**: interactive to create an inmersive experience from the beggining (background music)
- **Feed Editor Page**: Display the selected image and using the json file, place the buttons in the correcponding coordinates, play sounds when the user press the button.
- **Information Page**: Develop an specific page to show the key aspects of the JWT mission.
- **Scroll Page**: create an special page that is displayed with all the community videos and addtions that users create.

## WorkFlow

The app works following a series of steps:
1. The images are separately processed and the coordinates for the light bodies added to the correcponding json file. This should be performed by the app in further steps, 
for now, we as a team had to process the images manually.
```python
def find_light_zones(image_path, max_illumintation=150, min_area=100, max_distance=50):
    #read image
    image = cv2.imread(image_path)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Get the dimensions of the image
    height, width = image.shape[:2]

    # Apply filter to find light zones
    _, umbral = cv2.threshold(gray, max_illumintation, 255, cv2.THRESH_BINARY)

    # find the perimeter of the zones
    perimeter, _ = cv2.findContours(umbral, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # list to storage the size of the rectangles
    rectangle = []

    for per in perimeter:
        # calculate area
        area = cv2.contourArea(per)
        if area > min_area:
            # create a rectangle
            x, y, w, h = cv2.boundingRect(per)
            rectangle.append((x, y, w, h))
```
2. Add the resources to the StarSound App: generated json, images and sounds need to be added to the project, in the final stages, we where available to use web
storage in Firebase in order to retrieve the image files
3. In the main screen of the App, the user will see the title and three buttons, one with the image of the telescope to navigate to the Feed Editor page and create the mixed music, 
a second button to access the key JWT mission data and brief explanations, and the final button 'Explore' to see all the work created by the community
4. In the Feed Editor page, the user will see the image and the buttons will be placed automatically by the app using the coordinates, then the user is able to press the buttons,
familiarize with the sounds and start making their own music, as each button plays a note or a chord, the user has to make a try several times to achieve a great sound, inviting
to creativity and maybe, multiple image selection to try different sound patters.
5. Once the user is happy with the sounds, the user can record a short video of the pattern created, that will save the mixed short "song" created by the user and the image of the
JWT used, this video can be uploaded to the community.
6. The user can navigate to the community page to see all the videos created by other users, inspire and learn about the other images available.

## Value of the project

As developers, we have this benefits to the community by creating this project:

- Fosters creativity through a novel intersection of visual art and music. User is free to try as many possibilities with the images, the sounds and the random assign to the buttons
- Encourages interest in space exploration by incorporating educational content about the JWT. We add information and names, relevant to understand the importance of the mission.
- Provides an engaging platform for users to explore sound in an interactive, exploratory way.
- The envirorment is similar to many social media platforms, this allows the user to have a comfortable experience.
- Open to all ages. Children can stimulate by creating particular mixes, but adults can also try to create a unique short song using more complex patterns and logic series to press determinated buttons and combine sounds.

We hope that with the community implementation of the StarSound App, we can spark the curiosity, promote the creativity, build a community in the app, promote learning, and reemplace general content, with science scrolling content.

The platform is developed so users have to user their creativity and curiosity to create and explore the possibilities of the app, and also, make an approach to the JWT mission.

## Technical Sheet

---

### Tools
1. **VS Code**: we implemented this code editor because of the many applications and extensions available.
2. **Flutter**: the platform allows us to test and deploy the application in almost all the common community devices, even if we used only android at the end.
3. **Python**: we used this language to process the images and create the json files, also to implement the OpenCV features.
4. **Firebase**: we implemented this as storage method and to upload the community videos. We had several issues with the Android communication, so this feature is not completely implemented, 
but isused as main source for images and in future steops coud be the base of all the communication in the app.
5. **Discord**: platform to communicate with team members
6. **Canva**: web design of banners, presentations and some other resources

---

### Languages

1. **Dart**: main implementation of Flutter uses dart as language and framwork, the main fileas and screens where developed with dart.
2. **Python**: implementation of the OpenCV features, json generation and image processing
3. **Kotlin, Java, C++, JavaScript**: other main languages for the configuration of the project and gradle in different devices

---

### Software libraries or packages

- [Google Drive](https://drive.google.com/) - Store visuals and videos
- [multidex](https://developer.android.com/build/multidex#kts) - Manage database operations
- [Card swiper](https://pub.dev/packages/card_swiper/example) - not finally implemented
- [matplotlib](https://pypi.org/project/matplotlib/)
- [OpenCV](https://pypi.org/project/opencv-python/)
- [SoundPool](https://developer.android.com/reference/android/media/SoundPool#:~:text=SoundPool%20|%20Android%20Developers.%20Essentials.%20Gemini%20in%20Android%20Studio.%20Your) - to play sounds